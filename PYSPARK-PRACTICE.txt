DATAFRAME INITIALIZATION IN SPARK


from pyspark.sql import Spark session

spark=sparksession.builder.appname('Basics).getOrCreate()

df=spark.read.json('people.json')

df.show 

For shwoing schema

df.printschema()

from pyspark.sql.types import (Structfield ,String type ,integer type,struct type) 

data_schema =[structfield('age',IntegerType(),True,StructField('name',StringType(),True))]

df.withcolumn('double_age',df['age']*2.show())

use sql queries in spark

df.createoReplaceTempView('people')

results=spark.sql("select*from people)

results.show()




Initializing Spark 
SparkContext 
>>> from pyspark import SparkContext

INSPECT SPARK CONTEXT
>>> sc.version

Loading Data 
>>> rdd = sc.parallelize([('a',7),('a',2),('b',2)])

External Data 
>>> textFile = sc.textFile("/my/directory/â€¢.txt")

RETRIEVE RDD INFORMATION
>>> rdd.getNumPartitions()
>>> rdd3.max() 


SELECTING DATA
>>> rdd.collect() #Return a list with all RDD elements [('a', 7), ('a', 2), ('b', 2)]
FILTERING

>>>> rdd5.distinct().collect() #Return distinct RDD values['a' ,2, 'b',7]

ITERATING
>>> def g (x): print(x)>>> rdd.foreach(g) #Apply a function to all RDD elements('a', 7)('b', 2)('a', 2)

SORT 
>>> rdd2.sortByKey().collect() #Sort (key, value) 

REPARTIONING
>>> rdd.repartition(4) #New RDD with 4 partitions

SAVING
>>> rdd.saveAsTextFile("rdd.txt"
STOPPING
>>> sc.stop()

